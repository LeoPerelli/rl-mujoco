# rl-mujoco


# Policy Gradient algorithms
Policy Gradient algorithms take a different route compared to Value based algorithms. 
In value based algorithms, the policy is estimated based on the state or state-action value estimates. 
In policy gradient methods instead, the policy is estimated directly. 
As such, we have an explicit parametrization of the policy $\pi_{\theta}(a)$ which outputs a probability distribution over the action space.
This has a few advantages, in particular: (i) the policy definition can naturally handle multi-mode or stochastic policies while this is not straightforward for state-action value estimation where epsilon-greedy introduces stochasticity but the policies are generally single-mode on the action with the highest expected reward, (ii) ??.

Policy gradients are framed as a maximisation problem where the objective is a performance function $J$, generally defined as the value of some starting state $s_0$:
```math
J(\theta) = v_{\pi_{\theta}}(s_0)
```

Through the policy gradient theorem and additional manipulations, we find the REINFORCE objective:
```math
\nabla J(\theta) \propto \mathbb{E}[G_t \frac{\nabla \pi(a_t | s_t, \theta)}{\pi(a_t | s_t, \theta)}]
```
where $G_t$ is the return of the episode starting from timestep t. 
Basically, the REINFORCE gradient is an average of the policy gradients with a weighting which is: (i) proportional to the reward obtained along the trajectory and (ii) inversely proportional to the probability of the given action, as frequent actions would occur more in the trajectories hence would be over-represented compared to rare actions in the computation of expected value.

While this expression provides a maximisation target, the optimisation on $\theta$ has to be managed with care: since the trajectories are sampled from a distribution generated by a specific $\theta$, as you move away from that $\theta$ towards one which is optimal on the objective the validity of the objective itself is more and more dubious. Overall, it is common for policy gradient methods to take steps which are too big in the direction of this objective, and fall in steep performance regressions.

One way to prevent making excessive updates to $\theta$ is provided by trust-region methods, which adds a constraint on the maximization such that the KL divergence between the initial and updated policy is at most $\epsilon$.
The main drawback is that solving this constrained optimisation problem is quite expensive and ???

# PPO algorithm
PPO tackles the same problem of trust-region methods by introducing a clipped objective. Let:

```math
r = \frac{\pi(a_t | s_t, \theta)}{\pi(a_t | s_t, \theta_{old})}
```
Then, the PPO objective is defined as:
```math
\mathbb{E}[min(rA_t, clip(r, 1-\epsilon, 1+\epsilon) A_t)]
```
Basically, this definition is performs a clip on how much you can optimise in the positive sense, but no clip on the opposite direction. 

<img src="images/clipping.png" alt="drawing" width="600" style="display:block; margin:auto;"/>

Notably, compared to REINFORCE, PPO uses advantage estimation (in particular, Generalised Advantage Estimation) rather than return estimation. 
The full PPO objective also contains two additional terms:
- An exploratory term to avoid policy collapse, implemented as a bonus based on policy entropy. 
- A critic term, implemented as the MSE between the observed and predicted advantage. This generally requires a separate network from the policy network.

Then, the (parallel) PPO algorithm proceeds as:

<img src="images/algorithm.png" alt="drawing" width="600" style="display:block; margin:auto;"/>

# Generalised Advantage Estimation
Generalised advantage estimation (GAE) has the same conceptual origin as TD($\lambda$) methods: find the right tradeoff between bias and variance.
To estimate the return of a trajectory, one has different options: Monte Carlo estimation is unbiased, but high variance. To reduce variance, one can boostrap, and the simplest form of bootstrapping are one step temporal difference (TD) methods.
Bootstrapping however introduces bias, as the return is estimated as the reward from one step plus the state value of the next state, which in general can bias the estimate as the state value function is itself biased.
GAE is a way to smoothly interpolate between estimates which have different levels of bias and variance, providing a method which is more flexible in mediating them compared to making a discrete choice of using a specific n-step return estimation.
While the policy optimisation objective can vary based on the estimate of the return, the advantage function generally yields the lowest variance estimate, as by definition it only measures how good or bad an action is with respect to the default behaviour of the policy. 
The advantage function is defined as:
```math
A_{\pi}(a, s) = Q_{\pi}(a, s) - V_{\pi}(s)
```
Using the advantage function means that the policy objective points to the direction of increased probability for the given action if and only if the action receives a better than average return compared to the state's value function.
The state value function is thus used as "baseline" against which we measure the actual return, and indeed baselines are used to reduce variance.
Let the k-step advantage be defined as:
```math
A_t^{k} = \sum_{l=0}^{k-1} \gamma^l\delta_{t+l} = -V(s_t) + r_t + \gamma r_{t+1} + ... + \gamma^{k-1}r_{t+k-1} + \gamma^{k}V(s_{t+k})
```
with $\gamma$ a discounting factor (which already introduces bias as it downweighs the future returns) and $\delta_t$ the one-step TD return at step t.
Then, the generalised advantage estimator is:
```math
A_t^{GAE(\gamma, \lambda)} = (1-\lambda)\sum_k \lambda^kA_t^{k} = \sum_k (\gamma \lambda)^k \delta_{t+k}
```
which is the exponentially weighted average of the k-step advantage.
Note that for $\lambda=0$ you recover the one-step TD estimate, while for $\lambda=1$ you recover the discounted monte carlo reward.

# Problem setup

## Policy parametrization
We consider two environments, the Hopper and Half cheetah. 
Both require outputting actions which correspond to continuous values (bounded in [-1,1]) representing the inputs to the motors of the joints.
The output has shape 3 for Hopper, as there are 3 motors to control. 
Since the policy has to define a probability density over the action space [-1,1], the policy outputs a mean and a variance of a gaussian distribution.
The gaussian distribution has domain over [-inf, +inf], so it has to undergo a transformation.
In particular, the policy outputs a log-std and a mean, which both can range on [-inf, +inf].
We can then sample on this space from the $N(\mu, \sigma^2)$ and transform this into the range [-1,1] through a hyperbolic tangent transformation.

This parametrisation requires a bit of care when computing the $r$ ratio.
Since we have to compute the probability of an action over the range [-1,1], we have to follow the transformation to reach the normal distribution.
Moreover, an action is a multi-dimensional vector, so we have to use the vector formula for the change of variables. 
In particular, if $y = g(x)$, where x and y are vectors, we have that:
```math
P_Y(y) = P_X(g^{-1}(y))|det(\frac{dg^{-1}(y)}{dy})|
```
Since x and y are vectors, g is a vector-to-vector mapping and we have to compute its jacobian. 
However, the vector components are independent, so the jacobian is a diagonal matrix and its determinant is the product of the diagonal components, which are the component-wise derivatives of the transformation, which is the same for all and is the hyperbolic tangent.

## PPO computations
We use a parallelized environment where, in general, different episodes proceed independently: at the same time, different episodes will be at different timesteps. 
Moreover, we update the policy every N timesteps, which means the data from the last N timesteps can contain any number K of episodes, where K differs between the different environments.
Most likely, the first and last episode provided as input to the policy updates will be incomplete (first one will have only the final part, while the last one will have the first part).
Therefore, it is important to manage the start/end boundaries, as well as the inter-episode boundaries. 
In our case, the one-step TD errors are computed in parallel as they can be obtained through matrix operations, while the generalised advantage estimator requires looping on the matrix backwards in time to accumulate and discount the one-step TD errors, resetting the accumulation when episode boundaries are hit.

# Results
We evaluate TBD runs on the Hopper and HalfCheetah environments, with fixed hyperparameters provided in the experiments folder. 
The only difference we test in terms of hyperparameters is the effect of normalising the GAE, and we evaluate the impact of this normalisation on the variance of TBD.
We run for TBD timesteps in each environment using TBD parallel environments, which on a M1 Pro Mac require roughly TBD hours.
The policy is updated every TBD steps, with TBD training epochs using the Adam optimiser.
Below are results for the runs:


Other interesting implementation details such as value function clipping (similar to the PPO objective for the policy updates), orthogonal initialization of neural networks and other types of normalization are explored in: https://arxiv.org/pdf/2005.12729.