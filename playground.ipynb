{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16a4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "envs = gym.make_vec(\"Hopper-v5\", num_envs=16, vectorization_mode=\"sync\") #wrappers=(gym.wrappers.TimeAwareObservation,))\n",
    "# envs = gym.wrappers.vector.ClipReward(envs, min_reward=0.2, max_reward=0.8)\n",
    "state, info = envs.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad898831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_position': array([ 0.00475192, -0.00460625,  0.00347442,  0.00489697,  0.00077178,\n",
       "         0.00047879,  0.00226332, -0.0048374 , -0.00049919,  0.00095829,\n",
       "        -0.00482584,  0.00281093,  0.00206797, -0.00012012, -0.0031364 ,\n",
       "        -0.00110096]),\n",
       " '_x_position': array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True]),\n",
       " 'z_distance_from_origin': array([-9.81253807e-04, -1.72343394e-03, -4.03278180e-03,  5.33785591e-04,\n",
       "         8.52779870e-04, -2.80178007e-03,  4.78320978e-03, -4.52251611e-04,\n",
       "         2.50688871e-03, -4.61085739e-03, -2.14853478e-03,  3.28168344e-04,\n",
       "        -2.03319339e-03,  4.18351843e-04,  1.38730193e-05, -3.67001202e-03]),\n",
       " '_z_distance_from_origin': array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d35735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24901875e+00, -2.92719788e-03,  1.78350440e-03,\n",
       "         3.59902052e-03,  4.55040301e-03, -1.04846590e-03,\n",
       "        -3.68232119e-03, -3.76634180e-03, -2.63891603e-03,\n",
       "        -2.47268295e-03,  1.37719765e-03],\n",
       "       [ 1.24827657e+00, -5.90692855e-04, -1.04681955e-04,\n",
       "         3.96376577e-03, -1.25008890e-04, -3.44829341e-03,\n",
       "        -1.98484911e-03, -3.73579778e-04,  7.23529430e-04,\n",
       "        -4.74048781e-04,  4.56532750e-03],\n",
       "       [ 1.24596722e+00,  4.79460478e-03,  8.45201218e-04,\n",
       "        -3.33799854e-03, -1.50665248e-03, -2.61922001e-03,\n",
       "         2.46431914e-03,  2.05568018e-03, -2.73507910e-03,\n",
       "         4.08493888e-04, -4.68815389e-03],\n",
       "       [ 1.25053379e+00,  3.91502955e-03, -3.55490049e-04,\n",
       "         1.28578234e-03, -1.20361303e-03, -2.79918184e-03,\n",
       "         4.65956414e-03, -3.83518384e-04, -3.68811028e-03,\n",
       "        -3.38385633e-04, -1.63786365e-03],\n",
       "       [ 1.25085278e+00,  4.10798896e-03,  1.35187891e-03,\n",
       "        -2.42922029e-03,  4.70743625e-03,  4.99902775e-03,\n",
       "         2.18082521e-03, -4.95724348e-03,  1.56825998e-03,\n",
       "         2.05483645e-03, -1.35066073e-03],\n",
       "       [ 1.24719822e+00, -4.38951928e-03,  3.99636684e-03,\n",
       "         4.03390065e-03, -3.02498018e-03, -3.53885831e-03,\n",
       "        -1.10993069e-03,  1.09267194e-04,  3.24723632e-03,\n",
       "         3.30221894e-03, -2.19318869e-03],\n",
       "       [ 1.25478321e+00,  3.91596785e-03,  3.83222432e-03,\n",
       "         3.10779998e-03,  3.65592893e-03,  4.75202748e-03,\n",
       "        -3.87705771e-03,  4.02299190e-03, -4.50950854e-03,\n",
       "        -1.70007335e-03,  2.53805493e-03],\n",
       "       [ 1.24954775e+00,  5.62525783e-04,  1.54336426e-03,\n",
       "         1.05824240e-03,  2.62247982e-03,  3.51960581e-03,\n",
       "         2.51748490e-03, -3.72426808e-03,  2.33744736e-03,\n",
       "         1.25638695e-03, -3.54611836e-03],\n",
       "       [ 1.25250689e+00,  8.58864852e-04, -4.17202699e-03,\n",
       "         3.44806571e-03, -4.09269601e-03, -3.25178011e-03,\n",
       "         1.88476808e-04,  1.27694782e-03, -4.63086603e-03,\n",
       "         1.21761684e-03,  3.72681450e-03],\n",
       "       [ 1.24538914e+00,  3.25419477e-03,  1.57995912e-03,\n",
       "         1.18668339e-03, -2.55138509e-03, -1.41986033e-03,\n",
       "         1.02871668e-03, -1.02692471e-03,  4.34763916e-03,\n",
       "        -1.63160141e-03,  1.30368855e-03],\n",
       "       [ 1.24785147e+00, -2.17300103e-04, -1.49881977e-04,\n",
       "        -4.38540873e-03,  2.61381772e-03, -2.30070710e-03,\n",
       "        -3.45035782e-03, -2.97137595e-03, -3.15125507e-04,\n",
       "         3.20403342e-03,  3.20827945e-03],\n",
       "       [ 1.25032817e+00, -3.18029971e-03,  3.61615075e-03,\n",
       "         3.49064576e-03, -1.59529704e-04,  4.00756894e-03,\n",
       "         2.07805643e-03, -8.58321216e-04, -1.40532360e-03,\n",
       "        -2.91617266e-03,  2.21836104e-03],\n",
       "       [ 1.24796681e+00,  1.89664725e-03,  3.07179791e-03,\n",
       "        -1.04078088e-03,  6.21764403e-04, -4.01846353e-03,\n",
       "        -3.35750438e-03,  4.60435069e-03,  2.70001410e-03,\n",
       "         2.39326934e-03,  6.10811035e-04],\n",
       "       [ 1.25041835e+00, -4.83373495e-03,  4.25449407e-03,\n",
       "        -3.62353424e-03, -1.29378893e-03,  1.62643770e-03,\n",
       "         1.93062904e-03,  2.72296403e-03, -2.39103201e-04,\n",
       "        -1.28773319e-04, -1.81833095e-03],\n",
       "       [ 1.25001387e+00,  2.66339380e-03,  2.14260437e-03,\n",
       "        -2.09133120e-03, -3.35916616e-03,  4.72018357e-03,\n",
       "         2.97006519e-03,  4.39314960e-03, -4.29060685e-04,\n",
       "        -1.54734607e-03,  2.29700954e-03],\n",
       "       [ 1.24632999e+00, -4.38911526e-03, -3.29793377e-03,\n",
       "        -2.81649459e-03, -4.69249087e-03,  3.02968685e-03,\n",
       "        -1.93903475e-03,  4.82093954e-03,  9.29277796e-05,\n",
       "         1.71966486e-03,  3.50093173e-03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81425ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55eb920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "actions = np.zeros(shape=(16, 3))\n",
    "states_, rewards, terminateds, truncateds, infos = envs.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ffb3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24867795e+00, -3.14333322e-03,  1.66035217e-03,\n",
       "         3.36463696e-03,  4.57518629e-03, -6.51155179e-03,\n",
       "        -8.16026811e-02, -4.42285005e-02, -2.48293770e-02,\n",
       "        -4.91677975e-02,  4.36602792e-03],\n",
       "       [ 1.24794934e+00, -6.87785183e-04, -7.70808458e-05,\n",
       "         3.71726844e-03, -7.51970825e-05, -8.01835956e-03,\n",
       "        -7.99061894e-02, -2.08714198e-02,  5.46510007e-03,\n",
       "        -5.33221180e-02,  7.45015273e-03],\n",
       "       [ 1.24567311e+00,  4.77500037e-03,  7.78778752e-04,\n",
       "        -3.33106999e-03, -1.54351307e-03, -3.06664777e-03,\n",
       "        -7.59940926e-02, -5.74084016e-03, -1.23673220e-02,\n",
       "         1.19833804e-03, -4.54433981e-03],\n",
       "       [ 1.25025798e+00,  3.88171912e-03, -3.77851239e-04,\n",
       "         1.20483199e-03, -1.21234946e-03, -4.26898310e-03,\n",
       "        -7.36388409e-02, -6.96614258e-03, -2.13199687e-03,\n",
       "        -1.73712142e-02, -6.87339201e-04],\n",
       "       [ 1.25055651e+00,  3.99762755e-03,  1.27708454e-03,\n",
       "        -2.40562751e-03,  4.69766867e-03,  4.11219742e-03,\n",
       "        -7.62547767e-02, -2.03873088e-02, -1.74903141e-02,\n",
       "         3.61186930e-03, -1.12312553e-03],\n",
       "       [ 1.24687889e+00, -4.72239815e-03,  3.76876747e-03,\n",
       "         3.80420638e-03, -3.02380640e-03, -1.16396621e-02,\n",
       "        -7.88360816e-02, -7.27236999e-02, -5.20902090e-02,\n",
       "        -5.25859484e-02,  1.88697494e-03],\n",
       "       [ 1.25444113e+00,  3.67265218e-03,  3.57288820e-03,\n",
       "         2.90816577e-03,  3.69030644e-03, -1.56731127e-03,\n",
       "        -8.17366746e-02, -5.59061391e-02, -5.30542293e-02,\n",
       "        -4.22052711e-02,  5.59361825e-03],\n",
       "       [ 1.24925501e+00,  4.15635227e-04,  1.46117084e-03,\n",
       "         1.00018434e-03,  2.59969399e-03,  9.72142641e-04,\n",
       "        -7.57363045e-02, -2.93004209e-02, -1.97002579e-02,\n",
       "        -1.36188185e-02, -2.32542501e-03],\n",
       "       [ 1.25219679e+00,  7.85657904e-04, -4.18956028e-03,\n",
       "         3.24237789e-03, -4.05107513e-03, -7.30792238e-03,\n",
       "        -7.77868503e-02, -1.69050088e-02, -3.80688918e-04,\n",
       "        -4.57294472e-02,  6.29210160e-03],\n",
       "       [ 1.24508455e+00,  3.12234123e-03,  1.50629459e-03,\n",
       "         1.10526009e-03, -2.53536475e-03, -4.05893001e-03,\n",
       "        -7.72116561e-02, -2.80371136e-02, -1.93822866e-02,\n",
       "        -1.64970073e-02,  2.51855227e-03],\n",
       "       [ 1.24750994e+00, -2.41097038e-04, -1.52384926e-04,\n",
       "        -4.35985866e-03,  2.63939629e-03, -2.30282784e-03,\n",
       "        -8.19297658e-02, -2.97785330e-03, -3.10619542e-04,\n",
       "         3.18350598e-03,  3.18638853e-03],\n",
       "       [ 1.25003373e+00, -3.46775110e-03,  3.38682544e-03,\n",
       "         3.26091859e-03, -1.26608832e-04, -2.66348865e-03,\n",
       "        -7.57823733e-02, -6.19451827e-02, -4.88976195e-02,\n",
       "        -4.78274865e-02,  5.51318557e-03],\n",
       "       [ 1.24762650e+00,  1.77469459e-03,  2.89721724e-03,\n",
       "        -1.00549358e-03,  6.28883648e-04, -6.00946427e-03,\n",
       "        -8.17345442e-02, -3.00324632e-02, -4.00894145e-02,\n",
       "         5.90675174e-03,  1.09684555e-03],\n",
       "       [ 1.25012043e+00, -5.02334143e-03,  3.99136409e-03,\n",
       "        -3.60298614e-03, -1.30528693e-03, -1.01926976e-03,\n",
       "        -7.64297859e-02, -4.33144577e-02, -5.71241976e-02,\n",
       "         4.56351270e-03, -1.15349431e-03],\n",
       "       [ 1.24972402e+00,  2.59275682e-03,  2.00843459e-03,\n",
       "        -2.09286527e-03, -3.33935956e-03,  3.39743159e-03,\n",
       "        -7.54431778e-02, -1.86381171e-02, -2.88924088e-02,\n",
       "         8.11717753e-04,  2.60592083e-03],\n",
       "       [ 1.24600056e+00, -4.35056780e-03, -3.29718775e-03,\n",
       "        -2.80277813e-03, -4.66458150e-03,  3.02825556e-03,\n",
       "        -8.04185027e-02,  4.81593008e-03,  9.35761188e-05,\n",
       "         1.70945974e-03,  3.47644027e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c48efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not terminateds.any():\n",
    "    states_, rewards, terminateds, truncateds, infos = envs.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b96cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False  True\n",
      " False False False False]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "self._autoreset_envs=array([False, False, False, False, False, False, False, False, False,\n       False, False,  True, False, False, False, False])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(terminateds)\n\u001b[0;32m----> 2\u001b[0m states_, rewards, terminateds, truncateds, infos \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(terminateds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py:268\u001b[0m, in \u001b[0;36mSyncVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    259\u001b[0m         (\n\u001b[1;32m    260\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_obs[i],\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m             env_info,\n\u001b[1;32m    265\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[i]\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoreset_mode \u001b[38;5;241m==\u001b[39m AutoresetMode\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# assumes that the user has correctly autoreset\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autoreset_envs[i], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autoreset_envs\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     (\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_obs[i],\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m         env_info,\n\u001b[1;32m    275\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[i]\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoreset_mode \u001b[38;5;241m==\u001b[39m AutoresetMode\u001b[38;5;241m.\u001b[39mSAME_STEP:\n",
      "\u001b[0;31mAssertionError\u001b[0m: self._autoreset_envs=array([False, False, False, False, False, False, False, False, False,\n       False, False,  True, False, False, False, False])"
     ]
    }
   ],
   "source": [
    "print(terminateds)\n",
    "states_, rewards, terminateds, truncateds, infos = envs.step(actions)\n",
    "print(terminateds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cbdc412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncateds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a2ef7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3206017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from agent import PPOPolicy, PPOPolicyBaseline\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, num_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate the agent's performance with epsilon=0 (no exploration).\n",
    "    \"\"\"\n",
    "\n",
    "    total_rewards = []\n",
    "    total_lengths = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            state_, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            state = state_\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_lengths.append(episode_length)\n",
    "\n",
    "    avg_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_length = sum(total_lengths) / len(total_lengths)\n",
    "\n",
    "    return avg_reward, avg_length\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gym.register_envs(ale_py)\n",
    "\n",
    "    n_steps = 1000000\n",
    "    lr_actor = 1e-3  # seems to be the minimum that works\n",
    "    device = torch.device(\"mps\")\n",
    "    eval_frequency = 50000\n",
    "    eval_episodes = 5\n",
    "    gamma = 0.95\n",
    "\n",
    "    agent = PPOPolicyBaseline(device=device, lr_actor=lr_actor, gamma=gamma)\n",
    "\n",
    "    s0 = torch.tensor([-0.04864075, 0.0167673, -0.00097064, -0.04351403])\n",
    "    s1 = torch.tensor([0.0, 0.0, 0.2, 0.2])\n",
    "    s2 = -torch.tensor([-0.04864075, 0.0167673, -0.00097064, -0.04351403])\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    writer = SummaryWriter(log_dir=f\"runs/ppo_{int(time.time())}\")\n",
    "    state, info = env.reset()\n",
    "    end = False\n",
    "    running_episodes = 0\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    episode_loss = 0\n",
    "    episode_grad_norm_actor = 0\n",
    "    episode_grad_norm_critic = 0\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    ends = []\n",
    "    for i in tqdm(range(n_steps)):\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        state_, reward, terminated, truncated, info = env.step(action)\n",
    "        end = terminated or truncated\n",
    "\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        ends.append(end)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if i % 50 == 0:\n",
    "\n",
    "            for enum, s in enumerate([s0, s1, s2]):\n",
    "                debug_policy = torch.softmax(\n",
    "                    agent.policy.forward(s),\n",
    "                    dim=-1,\n",
    "                )[0]\n",
    "\n",
    "                debug_baseline = agent.baseline.forward(s)[0].detach()\n",
    "\n",
    "                writer.add_scalar(\n",
    "                    f\"DebugPolicy/s{enum}\",\n",
    "                    debug_policy,\n",
    "                    i,\n",
    "                )\n",
    "\n",
    "                writer.add_scalar(\n",
    "                    f\"DebugBaseline/s{enum}\",\n",
    "                    debug_baseline,\n",
    "                    i,\n",
    "                )\n",
    "\n",
    "        if end:\n",
    "            state, info = env.reset()\n",
    "            end = False\n",
    "\n",
    "            if running_episodes % 5 == 0:\n",
    "                episode_loss, clip_loss, mse_loss, entropy_loss = agent.update_policy(\n",
    "                    rewards, states, actions, ends\n",
    "                )\n",
    "\n",
    "                rewards = []\n",
    "                actions = []\n",
    "                states = []\n",
    "                ends = []\n",
    "\n",
    "            writer.add_scalar(\"Reward/episode\", episode_reward, running_episodes)\n",
    "            writer.add_scalar(\"Episode/Length\", episode_length, running_episodes)\n",
    "            writer.add_scalar(\"Episode/Loss\", episode_loss, running_episodes)\n",
    "            writer.add_scalar(\"Episode/clip_loss\", clip_loss, running_episodes)\n",
    "            writer.add_scalar(\"Episode/mse_loss\", mse_loss, running_episodes)\n",
    "            writer.add_scalar(\"Episode/entropy_loss\", entropy_loss, running_episodes)\n",
    "            writer.add_scalar(\n",
    "                \"DebugGrad/Episode Grad Norm actor\",\n",
    "                episode_grad_norm_actor / episode_length,\n",
    "                running_episodes,\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"DebugGrad/Episode Grad Norm critic\",\n",
    "                episode_grad_norm_critic / episode_length,\n",
    "                running_episodes,\n",
    "            )\n",
    "\n",
    "            running_episodes += 1\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            episode_grad_norm_actor = 0\n",
    "            episode_grad_norm_critic = 0\n",
    "\n",
    "        else:\n",
    "            state = state_\n",
    "\n",
    "        if i % eval_frequency == 0 and i > 0:\n",
    "            print(f\"Evaluating agent at step {i} and saving model...\")\n",
    "            torch.save(agent.policy.state_dict(), f\"policy_gradient_model_{i}.pth\")\n",
    "\n",
    "            # Create a separate environment for evaluation to avoid affecting training\n",
    "            eval_env = gym.make(\"CartPole-v1\")\n",
    "            avg_reward, avg_length = evaluate_agent(\n",
    "                agent, eval_env, num_episodes=eval_episodes\n",
    "            )\n",
    "\n",
    "            writer.add_scalar(\"Evaluation/Average Reward\", avg_reward, i)\n",
    "            writer.add_scalar(\"Evaluation/Average Episode Length\", avg_length, i)\n",
    "\n",
    "            print(\n",
    "                f\"Evaluation results - Avg reward: {avg_reward:.2f}, Avg episode length: {avg_length:.2f}\"\n",
    "            )\n",
    "            eval_env.close()\n",
    "\n",
    "    print(f\"Total episodes: {running_episodes}\")\n",
    "    # After training loop ends:\n",
    "    torch.save(agent.policy.state_dict(), \"policy_gradient_model_last.pth\")\n",
    "\n",
    "    # agent.state_encoder.load_state_dict(torch.load(\"dqn_model.pth\"))\n",
    "    # agent.state_encoder.to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
